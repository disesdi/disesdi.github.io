-------

**home** \| <a href="https://anglesofattack.io/resources.html" target="_blank" rel="noopener noreferrer">resources</a> \| <a href="https://github.com/disesdi/" target="_blank" rel="noopener noreferrer">github</a> \| <a href="https://anglesofattack.io/about.html" target="_blank" rel="noopener noreferrer">about</a>

-------

# Angles of Attack
*Safety-Critical AI Systems (SCAIS) Engineering*

-------

## AI Security Publications & Research

-------

### <a href="https://zenodo.org/records/13905972" target="_blank" rel="noopener noreferrer">Securing AIML Systems in the Age of Information Warfare >> </a> 

A novel AI/machine learning security operations (AI/MLSecOps) architecture, including systems for operationalized security, auditing, data threat, and information warfare detection, along with OODA Loop-based game theoretic modeling of information warfare in AI/ML systems, and boolean path threat modeling & cyber resiliency metrics adapted to the canonical MLOps development cycle. <a href="https://zenodo.org/records/13905972" target="_blank" rel="noopener noreferrer"> >> </a>

### <a href="https://zenodo.org/records/13905960" target="_blank" rel="noopener noreferrer">AI-DAL: Towards Security Design Assurance for Artificial Intelligence Systems in Production >> </a>

The rise of artificial intelligence applications in society, and their accompanying security concerns, has created a need for regulatory oversight that is auditable, actionable, and adaptable to a rapidly changing technological landscape. Methods from safety-critical software engineering, particularly aerospace, are adapted to use in production AIML to aid both practitioners and regulators in establishing design thresholds for AIML system security. Assignment of AI Design Assurance Levels (AI-DAL) to projects/components, along with production of related compliance artifacts, is proposed as a means of consistently applying appropriate design requirements based on a systemâ€™s potential adverse impact. <a href="https://zenodo.org/records/13905960" target="_blank" rel="noopener noreferrer"> >> </a>

### <a href="https://github.com/disesdi/mlsecops_references" target="_blank" rel="noopener noreferrer">MLSecOps Reference Repository >> </a>

A repo for some of the most influential papers, books, & other media on machine learning operations (MLOps), adversarial machine learning, AIML policy and compliance, ++ 
<a href="https://github.com/disesdi/mlsecops_references" target="_blank" rel="noopener noreferrer"> >> </a>

-------

## Code

-------

### [Evaluating Robustness of Physical Unclonable Functions (PUFs) for Unmanned Aerial System Authentication via Random Forests & Gradient Boosting >>](https://disesdi.github.io/1/pufs.html)

PUFs have been proposed as a lightweight drone authentication method resistant to man-in-the-middle attacks. But how secure are they against machine learning? [ >> ](https://disesdi.github.io/1/pufs.html)

-------

### [Auto Axelrod Tournament Generator >>](https://github.com/disesdi/auto_axelrod)

A python game theory project to auto-generate multi-strategy Axelrod tournaments, & visualize their results [ >> ](https://github.com/disesdi/auto_axelrod)

-------

## Other Projects

-------

### <a href="https://github.com/indigenousEngineering" target="_blank" rel="noopener noreferrer">Indigenous.Engineering >> </a>

Data science, NLProc, and AIML for the culture <a href="https://github.com/indigenousEngineering" target="_blank" rel="noopener noreferrer"> >> </a>

-------

**home** \| <a href="https://anglesofattack.io/resources.html" target="_blank" rel="noopener noreferrer">resources</a> \| <a href="https://github.com/disesdi/" target="_blank" rel="noopener noreferrer">github</a> \| <a href="https://anglesofattack.io/about.html" target="_blank" rel="noopener noreferrer">about</a>

-------

<div align="center">ðŸ•·</div>
